Our work builds on Feng et al's (2021) research, which introduced novel methods to quantify political bias in large language models. We fine-tuned 6 RoBERTa models over two dimensions: political leaning (left, center, right) and domain (news, social media). Due to computational limits, we unfortunately were only able to fine tune each model for 3 epochs. Then, we gave each model the political compass test to understand how fine-tuning had impacted the model's political leanings.